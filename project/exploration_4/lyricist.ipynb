{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fba5147",
   "metadata": {},
   "source": [
    "# 작사가 만들기\n",
    "\n",
    "## 데이터 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b088889",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_corpus = []\n",
    "path_folder = './data/lyrics/'\n",
    "path_files = glob.glob(path_folder + '*.txt')\n",
    "\n",
    "for path_file in path_files:\n",
    "    with open(path_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2c635",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ad683",
   "metadata": {},
   "source": [
    "### 샘플 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94fbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 단어가 없는 문장 제거\n",
    "# 2. 동일 단어 인식 (소문자로 인식) \n",
    "# 3. 특수문자 분리\n",
    "# 4. 여러공백 하나로 처리\n",
    "# 5. <start>, <end> 추가\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c56ea635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264c4c3",
   "metadata": {},
   "source": [
    "### 토큰화, 시퀀스로 변환(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9aaa05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # 출연하는 단어들에 숫자를 매핑 한다.\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 단어장을 사용하여 문장을 매핑된 순자로 바꾸고, 리스트 형식으로 변환하여 Tensor로 만든다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기\n",
    "    idx_tensor = []\n",
    "    for idx, sentence in enumerate(tensor):\n",
    "        if len(sentence) > 15:\n",
    "            idx_tensor.append(idx)\n",
    "    idx_tensor = idx_tensor[::-1]\n",
    "    \n",
    "    for idx in idx_tensor:\n",
    "        del tensor[idx]\n",
    "        del corpus[idx]\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    word_index = tokenizer.word_index\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71763d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eada4a",
   "metadata": {},
   "source": [
    "#### 토큰, 시퀀스 확인하기 (사전)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de46b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "\n",
      "\n",
      "copus[0]\n",
      " ['<start> now i ve heard there was a secret chord <end>']\n",
      "\n",
      "\n",
      "tensor[0]\n",
      " [[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('token')\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break\n",
    "        \n",
    "print('\\n')\n",
    "print('copus[0]\\n', corpus[:1])\n",
    "print('\\n')\n",
    "print('tensor[0]\\n', tensor[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be95bc9",
   "metadata": {},
   "source": [
    "## train data 생성, sample, label 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde95fa4",
   "metadata": {},
   "source": [
    "언어 모델의 입력 문장 :  <start> 나는 밥을 먹었다\n",
    "언어 모델의 출력 문장 : 나는 밥을 먹었다 <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f1fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca9e420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor:  156013\n",
      "\n",
      "enc_train:  (124810, 14)\n",
      "enc_val:  (31203, 14)\n",
      "dec_train:  (124810, 14)\n",
      "dec_val:  (31203, 14)\n",
      "[[   2   50    5   91  297   65   57    9  969 6042]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329]\n",
      " [   2   36    7   37   15  164  282   28  299    4]]\n",
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "print('\\ntensor: ', len(tensor))\n",
    "\n",
    "print('\\nenc_train: ', enc_train.shape)\n",
    "print('enc_val: ', enc_val.shape)\n",
    "print('dec_train: ', dec_train.shape)\n",
    "print('dec_val: ', dec_val.shape)\n",
    "\n",
    "print(tensor[:3, :10])\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654650b7",
   "metadata": {},
   "source": [
    "### batch 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e038be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# src_input array와 tgt_intput array 매칭\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "# 무작위로 섞은 BUFFER_SIZE개짜리 데이터셋 생성 \n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "# BATCH_SIZE 만큼씩 나눠서 Dataset 생성, drop_remainder=True는 나누다 남은것을 버리지 않고 Dataset으로 남긴다\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1cf1db",
   "metadata": {},
   "source": [
    "## 모델 설계\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ff7966",
   "metadata": {},
   "outputs": [],
   "source": [
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e28ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [-1.65608595e-04,  2.19021313e-04,  5.94105630e-04, ...,\n",
       "          7.34932255e-05,  2.14834610e-04, -2.44826224e-04],\n",
       "        [-2.90239434e-04,  2.42251976e-04,  6.02376414e-04, ...,\n",
       "          2.84549635e-04,  2.75603699e-04, -4.55546222e-04],\n",
       "        ...,\n",
       "        [ 2.73650308e-04,  9.89516266e-04,  2.49688677e-03, ...,\n",
       "         -7.38517730e-04, -1.75156491e-03, -1.08589313e-03],\n",
       "        [ 4.42998338e-04,  9.43913474e-04,  2.28636758e-03, ...,\n",
       "         -6.50344358e-04, -1.47134683e-03, -1.24295824e-03],\n",
       "        [ 5.07265329e-04,  8.58019688e-04,  2.08122865e-03, ...,\n",
       "         -4.74725122e-04, -7.84911623e-04, -1.34668220e-03]],\n",
       "\n",
       "       [[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [ 1.99995076e-04,  1.41418990e-04,  4.97752684e-04, ...,\n",
       "         -3.06102913e-04,  1.57927760e-04, -1.26307714e-04],\n",
       "        [ 6.39577513e-04,  4.44469042e-05,  5.55419188e-04, ...,\n",
       "         -6.70610752e-04,  1.31342633e-04, -8.68327334e-05],\n",
       "        ...,\n",
       "        [ 3.73763934e-04,  2.16318163e-04, -1.05736748e-04, ...,\n",
       "          2.63604306e-04, -7.99057540e-04,  4.64993063e-04],\n",
       "        [-1.00105637e-04, -7.56268491e-06, -2.84943322e-04, ...,\n",
       "          3.01551510e-04, -5.39488683e-04,  2.64870730e-04],\n",
       "        [-2.60474568e-04,  1.53653527e-04, -1.18714939e-04, ...,\n",
       "          3.99406446e-04, -3.88143904e-04,  9.36588549e-05]],\n",
       "\n",
       "       [[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [-3.26356210e-04,  3.30518902e-04,  1.50000677e-04, ...,\n",
       "         -6.22446023e-05,  3.76508338e-04, -8.84877300e-05],\n",
       "        [-2.46018870e-04,  3.71225004e-04,  2.27361656e-04, ...,\n",
       "         -4.07838015e-05,  3.61906801e-04, -2.63031194e-04],\n",
       "        ...,\n",
       "        [-4.38004936e-04,  3.34414333e-04,  1.53864079e-04, ...,\n",
       "          2.25772252e-04,  1.21843041e-04, -1.71877025e-03],\n",
       "        [-6.96589588e-04,  6.57683529e-04, -1.00227524e-04, ...,\n",
       "          6.03936380e-04,  4.35465518e-05, -1.77900598e-03],\n",
       "        [-1.00069703e-03,  9.73728136e-04, -3.94341594e-04, ...,\n",
       "          1.01657712e-03, -3.49974725e-05, -1.80494320e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [-3.70430680e-05,  2.19352049e-04,  1.91383806e-04, ...,\n",
       "         -2.13312902e-04,  3.02440108e-04, -3.27265559e-04],\n",
       "        [ 1.66538128e-04,  4.99469606e-05, -1.23702732e-04, ...,\n",
       "         -2.48349475e-04,  3.63422558e-04, -4.33592213e-04],\n",
       "        ...,\n",
       "        [ 1.34546135e-03,  1.20834750e-03, -2.13941064e-04, ...,\n",
       "         -2.66717598e-05,  1.24791975e-03,  5.74588892e-04],\n",
       "        [ 9.26206645e-04,  1.54119427e-03, -3.70465277e-04, ...,\n",
       "          4.30316635e-04,  1.06799894e-03,  3.57026758e-04],\n",
       "        [ 4.33113775e-04,  1.86122209e-03, -5.81327942e-04, ...,\n",
       "          8.97916849e-04,  8.92107200e-04,  1.17878066e-04]],\n",
       "\n",
       "       [[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [-3.58857593e-04,  2.27617740e-04,  1.37701049e-04, ...,\n",
       "          5.94761004e-05,  1.24818122e-04, -1.20153309e-04],\n",
       "        [-5.38631110e-04, -1.80397035e-06, -8.03633375e-05, ...,\n",
       "          1.88113263e-04,  8.86954585e-05, -1.32540401e-04],\n",
       "        ...,\n",
       "        [ 1.86511737e-04,  4.84547054e-04, -2.48191296e-04, ...,\n",
       "          1.49653456e-03, -8.93686374e-04, -3.07027687e-04],\n",
       "        [-2.45695584e-04,  8.12303333e-04, -5.13882260e-04, ...,\n",
       "          1.82152493e-03, -9.09257855e-04, -5.07692865e-04],\n",
       "        [-6.63994346e-04,  1.13666023e-03, -7.79844762e-04, ...,\n",
       "          2.12746835e-03, -8.76090489e-04, -6.92753587e-04]],\n",
       "\n",
       "       [[-1.01212951e-04,  1.89399725e-04,  2.05378718e-04, ...,\n",
       "         -1.26890547e-04,  1.55438320e-04,  2.35424432e-05],\n",
       "        [-1.45819897e-04,  2.82658410e-04,  4.50850552e-04, ...,\n",
       "         -1.88547914e-04,  3.89262277e-05,  2.02340932e-04],\n",
       "        [ 6.94292030e-05,  4.15943578e-05,  6.26871421e-04, ...,\n",
       "         -6.37744233e-05,  1.96062480e-04,  6.72305818e-04],\n",
       "        ...,\n",
       "        [-1.17819977e-03,  5.59401815e-04,  4.88979567e-05, ...,\n",
       "          1.60844636e-03,  9.43004270e-04, -7.95395637e-04],\n",
       "        [-1.37021707e-03,  9.83465929e-04, -2.40394176e-04, ...,\n",
       "          1.99791254e-03,  6.36539189e-04, -1.00875436e-03],\n",
       "        [-1.55586889e-03,  1.38062099e-03, -5.39023313e-04, ...,\n",
       "          2.32928689e-03,  3.99881857e-04, -1.16988702e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "src_sample = None\n",
    "tgt_sample = None\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97c4db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ceff108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 98s 154ms/step - loss: 3.4076\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.9621\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.7871\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 103s 169ms/step - loss: 2.6552\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.5436\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.4439\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.3524\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 103s 169ms/step - loss: 2.2667\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 103s 169ms/step - loss: 2.1859\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.1095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75f996b040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a631b",
   "metadata": {},
   "source": [
    "loss : 2.1095"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff694a54",
   "metadata": {},
   "source": [
    "## test 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe797b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    5   22    9  158   80    4    5   22    9 5120    3]\n",
      "2\n",
      "5\n",
      "22\n",
      "9\n",
      "158\n",
      "80\n",
      "4\n",
      "5\n",
      "22\n",
      "9\n",
      "5120\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> i m a bad girl , i m a cunt <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다\n",
    "    print(test_tensor[0].numpy())\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        print(word_index)\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626aee6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f45264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  5 33  7  4  5 33  7  3]\n",
      "2\n",
      "5\n",
      "33\n",
      "7\n",
      "4\n",
      "5\n",
      "33\n",
      "7\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다\n",
    "    print(test_tensor[0].numpy())\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        print(word_index)\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fad9e3",
   "metadata": {},
   "source": [
    "최종 loss : 2.1095 <br>\n",
    "입력 sentence : 'i love'<br>\n",
    "출력 sentence : '<start> i love you , i love you <end> '<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
